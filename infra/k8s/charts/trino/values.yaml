
image:
  repository: trinodb/trino
  tag: latest
  pullPolicy: Always

server:
  workers: 1
  node:
    environment: trino
    dataDir: /data/trino
    pluginDir: /usr/lib/trino/plugin
  log:
    trino:
      level: INFO
  config:
    path: /etc/trino
    https:
      enabled: false
      port: 8443
      keystore:
        path: "usr/lib/trino/trino_keystore.jks"
    authenticationType: "PASSWORD"
    query:
      maxMemory: "256GB"
  exchangeManager:
    name: "filesystem"
    baseDir: gs://trino-lake-exchange-spooling
  autoscaling:
    enabled: true
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
    behavior: 
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
        - type: Percent
          value: 100
          periodSeconds: 15
      scaleUp:
        stabilizationWindowSeconds: 0
        policies:
        - type: Percent
          value: 800
          periodSeconds: 15
        - type: Pods
          value: 1
          periodSeconds: 15
        selectPolicy: Max


additionalConfigProperties:
  - internal-communication.shared-secret=super-secret-communication-shared-secret
  - http-server.process-forwarded=true
  - hide-inaccessible-columns=true

additionalCatalogs:
    connector.name=memory
  memory: |
    connector.name=memory
    memory.max-data-per-node=128MB

service:
  type: ClusterIP
  port: 8080
coordinator:
  jvm:
    maxHeapSize: "50G"
    gcMethod:
      type: "UseG1GC"
      g1:
        heapRegionSize: "32M"
  config:
    query:
      maxMemoryPerNode: "256GB"
  additionalJVMConfig:
    - -XX:+UnlockDiagnosticVMOptions
    - -XX:G1NumCollectionsKeepPinned=10000000
  resources:
    limits:
      cpu: 15
      memory: 256Gi
    requests:
      cpu: 7
      memory: 60Gi
  nodeSelector:
    nodegroupname: trino-node-pool
  tolerations:
   - key: "nodegroupname" 
     operator: "Equal"  
     value: "trino-node-pool"
     effect: "NoSchedule"
  livenessProbe:
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  readinessProbe:
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  additionalConfigFiles:
    is_coordinator.txt: |
      true
    ranger-trino-security.xml: |
      <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
      <configuration xmlns:xi="http://www.w3.org/2001/XInclude">
        <property>
          <name>ranger.plugin.trino.policy.rest.url</name>
          <value>https://ranger-hostname:6182</value>
          <description>MANDATORY: a comma separated list of URLs to Apache Ranger instances in a deployment</description>
        </property>
        <property>
          <name>ranger.plugin.trino.access.cluster.name</name>
          <value></value>
          <description>Name to identify the cluster running the Trino instance. This is recorded in audit logs generated by the plugin</description>
        </property>
        <property>
          <name>ranger.plugin.trino.use.rangerGroups</name>
          <value>false</value>
          <description>Boolean flag to specify whether user-to-groups mapping should be obtained from in Apache Ranger. Default: false</description>
        </property>
        <property>
          <name>ranger.plugin.trino.use.only.rangerGroups</name>
          <value>false</value>
          <description>Boolean flag. true: use only user-to-groups mapping from Apache Ranger; false: use user-to-groups mappings from Apache Ranger and Trino. Default: false</description>
        </property>
        <property>
          <name>ranger.plugin.trino.super.users</name>
          <value></value>
          <description>Comma separated list of user names. Superusers will be authorized for all accesses, without requiring explicit policy grants.</description>
        </property>
        <property>
          <name>ranger.plugin.trino.super.groups</name>
          <value></value>
          <description>Comma separated list of group names. Users in supergroups will be authorized for all accesses, without requiring explicit policy grants</description>
        </property>
      </configuration>  
    ranger-trino-audit.xml: |
      <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
      <configuration xmlns:xi="http://www.w3.org/2001/XInclude">
        <property>
          <name>xasecure.audit.is.enabled</name>
          <value>true</value>
          <description>Boolean flag to specify if the plugin should generate access audit logs. Default: true</description>
        </property>
        <property>
          <name>xasecure.audit.solr.is.enabled</name>
          <value>false</value>
          <description>Boolean flag to specify if audit logs should be stored in Solr. Default: false</description>
        </property>
        <property>
          <name>xasecure.audit.solr.solr_url</name>
          <value></value>
          <description>URL to Solr deployment where the plugin should send access audits to</description>
        </property>
      </configuration>
    ranger-policymgr-ssl.xml: |
      <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
      <configuration xmlns:xi="http://www.w3.org/2001/XInclude">
        <!-- properties used for 2-way SSL between the Trino plugin and Apache Ranger server -->
        <property>
          <name>xasecure.policymgr.clientssl.keystore</name>
          <value></value>
          <description>Path to keystore file. Only required for two-way SSL. This property should not be included for one-way SSL</description>
        </property>
        <property>
          <name>xasecure.policymgr.clientssl.keystore.type</name>
          <value>jks</value>
          <description>Type of keystore. Default: jks</description>
        </property>
        <property>
          <name>xasecure.policymgr.clientssl.keystore.credential.file</name>
          <value></value>
          <description>Path to credential file for the keystore; the credential should be in alias sslKeyStore. Only required for two-way SSL. This property should not be included for one-way SSL</description>
        </property>
        <property>
          <name>xasecure.policymgr.clientssl.truststore</name>
          <value></value>
          <description>Path to truststore file</description>
        </property>
        <property>
          <name>xasecure.policymgr.clientssl.truststore.type</name>
          <value>jks</value>
          <description>Type of truststore. Default: jks</description>
        </property>
        <property>
          <name>xasecure.policymgr.clientssl.truststore.credential.file</name>
          <value></value>
          <description>Path to credential file for the truststore; the credential should be in alias sslTrustStore</description>
        </property>
      </configuration>
worker:
  jvm:
    maxHeapSize: "50G"
    gcMethod:
      type: "UseG1GC"
      g1:
        heapRegionSize: "32M"
  config:
    memory:
      heapHeadroomPerNode: ""
    query:
      maxMemoryPerNode: "256GB"
  additionalJVMConfig:
    - -XX:+UnlockDiagnosticVMOptions
    - -XX:G1NumCollectionsKeepPinned=10000000
  resources:
    limits:
      cpu: 4
      memory: 256Gi
    requests:
      cpu: 1
      memory: 60Gi
  livenessProbe:
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  readinessProbe:
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  lifecycle:
    preStop:
      exec:
        command: ["/bin/sh", "-c", "curl -v -X PUT -d '\"SHUTTING_DOWN\"' -H \"Content-type: application/json\" http://localhost:8081/v1/info/state"]
  terminationGracePeriodSeconds: 30
  additionalConfigFiles:
    is_coordinator.txt: |
      false
ingress:
  enabled: true
  web:
    enabled: true
    annotations:
      alb.ingress.kubernetes.io/backend-protocol: HTTPS
      alb.ingress.kubernetes.io/listen-ports: '{"HTTPS": 443}, {"HTTP": 80}'
    path: "/"
    pathType: "ImplementationSpecific"
    host: "suzano-trino.prd.com.br"
    ingressClassName: alb

postgresql: